I should use 3D pose estimation, because it's better for tracking dance movements.
(Lifting is first finding 2D coordinates and the estimating a depth - this would also probably work for me.)

Tools:
Tool,Accuracy,Speed,Best For...
MediaPipe,High,Excellent,"Mobile apps, real-time web browsers, and single-dancer tracking."
YOLO11 Pose,Very High,High,"The current 2026 industry standard. Fast, robust against blurry motion, and handles multiple dancers well."
OpenPose,High,Low,"Academic research or setups with high-end GPUs. It’s the ""OG"" but very heavy."
MoveNet,Medium,High,Ultra-low-latency tracking on weak hardware (like old smartphones).

Skeleton keypoints:
- Core Joints: Hips, Shoulders, Spine.
- Extremities: Wrists, Ankles, Knees, Elbows.
- Confidence Scores: Every keypoint comes with a probability (0.0 to 1.0). If the score is low (e.g., $<0.5$), the joint is likely hidden; you should use "Temporal Smoothing" (interpolating from previous frames) to guess its location.

Normalizing for Comparison
- Translation Normalization (Centering)Move the dancer so their center of gravity (usually the midpoint of the hips) is at $(0, 0, 0)$.
	$$P_{normalized} = P_{original} - P_{hip\_center}$$
- Scale NormalizationA tall dancer and a short dancer should produce the same "score." Divide all coordinates by the length of the dancer's torso (distance between neck and mid-hip). This makes the skeleton unit-less.
	$$P_{scaled} = \frac{P_{translated}}{||P_{neck} - P_{hip\_center}||}$$
- Rotation Normalization (View Invariance)This is the hardest part. If Dancer A is facing the camera and Dancer B is at a 45° angle, their $(x, y)$ coordinates will be different even if they are doing the same move.The Fix: Rotate the 3D skeleton so the "Shoulder Line" is always parallel to the X-axis. This ensures you are always comparing the "Front View" of the dance.





packages for now:
- mediapipe: The AI model that finds the skeleton.
- opencv-python: Handles video files (opening, reading frames, saving output).
- numpy: The "math" library we use to normalize and shift coordinates.


free good quality photos and videos: Pexels, Pixabay

session starter: .\venv\Scripts\activate






pose estimation: mediapipe, task api

mediapipe is glitchy (even the heavy version), so YOLO11 is up for tryouts

yolo11: x -> most accurate, m -> medium, n -> fastest

back to mediapipe lol, it was better

pose estimation with mediapipe:
https://www.youtube.com/watch?v=06TE_U21FK4






Pro-Tips for ML Training:
- Normalization: Store your coordinates as values between 0 and 1 (normalized by width/height). This makes training much more stable.
- Confidence Scores: If your extraction tool (like MediaPipe or OpenPose) provides confidence scores, store them as the 3rd or 4th coordinate $(x, y, score)$. Your model will learn to "distrust" low-score joints.
- Data Types: Use float32 instead of float64. It cuts your file size in half with zero meaningful loss in precision for pose data.


question: how to store the medium sized dataset I use for training?

Best workflow for 50 videos:
1. Store everything in HDF5 for your ML training.
2. Create a small "Preview" API (using FastAPI or Flask) that reads the HDF5 file in Python and sends just the specific video's data to React as JSON.
3. Use React to render: Use a <canvas> or SVG to draw the lines between the $(x, y)$ coordinates.


// Example: Drawing a frame in React
const drawSkeleton = (ctx, keypoints) => {
  // keypoints = [[x1, y1], [x2, y2], ...]
  const connections = [[0, 1], [1, 2], [2, 3]]; // Define which joints connect
  connections.forEach(([start, end]) => {
    ctx.beginPath();
    ctx.moveTo(keypoints[start][0], keypoints[start][1]);
    ctx.lineTo(keypoints[end][0], keypoints[end][1]);
    ctx.stroke();
  });
};


This video demonstrates how the H5Web ecosystem allows you to explore and visualize HDF5 data directly within a web interface using performant WebGL components.
https://www.youtube.com/watch?v=wraFt4eCn8I

decided data format: HDF5 - best for medium sized datasets, stores everything in one binary file (catch: that file is not understandable)






how much data?

For a "Sanity Check" (Testing your code): 1–5 videos (~5,000 frames). The model will overfit (memorize), but you'll know your pipeline works.

For a Basic Recognizer (e.g., "Is this a jump or a turn?"): 20–50 videos. This is exactly where you are starting. If you have different people performing the same moves, the model will start to generalize.

For High-Level Performance (e.g., "Scoring a pro dancer"): 500+ videos. Large datasets like NTU RGB+D use over 56,000 sequences to reach 90%+ accuracy.

The "Golden Rule" for Dance: It's not just the number of videos; it's the diversity. 50 videos of the same person in the same room are less valuable than 20 videos of different people in different outfits/lighting.



strategies for dance data:

- Frame Rate Consistency: Ensure all videos are saved at the same FPS (e.g., 30 FPS). If one is 60 and one is 30, your model will think one dancer is moving twice as fast.
- Relative vs. Absolute: Don't just save raw pixel coordinates (e.g., $x=1080$). Save normalized coordinates ($0.0$ to $1.0$).
- The "Hip-Centric" Trick: For dance, it’s often helpful to subtract the "Mid-Hip" coordinate from all other points. This "centers" the dancer so the model learns the movements rather than where they are standing in the room.





If you are building a model to compare dancers (e.g., specific moves, style matching, or scoring), the Hybrid Approach is mathematically and conceptually superior to Skeletons alone.

Skeletons are mathematically precise but visually "thin."
- The "Stick Figure" Problem: A skeleton defines the angles of the bones, but it loses the volume of the dancer.
- Negative Space: In dance (form/ballet), the shape of the empty space between the arms and the body is critical. A skeleton only gives you the boundaries of that space (the joints), but the segmentation mask explicitly defines the entire filled volume.
- Robustness: Keypoint detectors often "jitter" (a knee point jumps 5 pixels). Segmentation masks are usually more stable over time (temporal coherence). If the skeleton glitches, the mask likely won't, providing a backup signal for your model.



When you train your model later, you will essentially have two "eyes":
1. The Mechanic (Skeleton): Checks if the knee angle is exactly 90 degrees.
2. The Artist (Mask): Checks if the overall silhouette matches the reference.

The Comparison Logic:
- Skeleton Metric: Euclidean distance between joints (Accuracy).
- Mask Metric: IoU (Intersection over Union) or Shape Embeddings.
- Example: Dancers A and B might have the exact same wrist position, but Dancer A has their shoulders hunched (changing the silhouette) while Dancer B has them down. The skeleton might miss the shoulder nuance if the neck keypoint is stable, but the mask will show a different "blob" shape around the neck/shoulders.



For your extractor and future model, I recommend the "Feature Embedding" Strategy.

A. Storage (The Extractor): Do not save full-resolution (1080p) masks. It is storage suicide and unnecessary for shape analysis.
- Action: Resize masks to 256x256 (or even 128x128). This preserves 99% of the relevant shape information (posture, limb thickness) while reducing file size by ~50x.
- Format: Save as compressed binary/integer maps.

B. The Model (Future Work): Instead of comparing raw pixels (which fails if the camera zooms in slightly), you will train a small generic ConvNet (Encoder).
- Input: 256x256 Mask.
- Output: A "Shape Vector" (e.g., 64 numbers) describing the silhouette.
- Comparison: You compare the Shape Vector of Dancer A vs. Dancer B. This vector is robust to small shifts but sensitive to posture changes.






DTW


Dynamic Time Warping (DTW) is absolutely essential for your project. In fact, for dance comparison, it is arguably the single most important algorithm you will use alongside the feature extraction.


1. The Problem: "The Timing Mismatch"

Imagine two dancers performing the exact same routine.

- Dancer A moves at a steady pace.
- Dancer B starts slightly slower, speeds up in the middle, and catches up at the end.

If you compare them frame-by-frame (Frame 1 vs Frame 1, Frame 10 vs Frame 10):
- At Frame 50, Dancer A might be raising their hand.
- At Frame 50, Dancer B might still be starting to raise their hand.

Result: Mathematically, the error is huge (Hand is up vs Hand is down). You tell Dancer B "Your form is wrong!", but their form is perfect—their timing was just different.


2. The Solution: "Elastic Matching"

DTW doesn't compare Frame[i] to Frame[i]. It looks for the best possible match forward or backward in time.

- It asks: "Which frame of Dancer B looks most like Frame 50 of Dancer A?"
- It finds that Frame 50 (A) matches Frame 54 (B).
- It "warps" the time axis (stretching and shrinking it like a rubber band) to align the moves perfectly.


3. Why it's perfect for your "Hybrid Approach"

You have two types of data now:

1. Skeleton (Vector Trajectories): Ideal for DTW. You run DTW on the skeleton joint angles. This gives you the "Alignment Path".
2. Masks (Visual Shapes):
	- Without DTW: You compare Mask A[50] vs Mask B[50]. (Mismatch).
	- With DTW: You use the "Alignment Path" map to compare Mask A[50] vs Mask B[54].

Result: You are now comparing the Same Pose against the Same Pose, even if they happened at different times.


4. Recommendation

- Phase 1 (Alignment): Use DTW on the Skeleton data (since it's low-dimensional and clean) to calculate the "Time Warp Path".
- Phase 2 (Scoring): Use that path to compare your Segmentation Masks and Trajectories. This allows you to separate "Timing Errors" (how much did I have to warp?) from "Form Errors" (even after warping, was the arm straight?).




For training, I need videos with fixed camera positions and zero moving if possible!!!





kell architektúra és design pattern + felhős tárolás

április elejére legyen kész a program

záróvizsga utánanézni

Dani ötlete: key moments kigyűjtése és összehasonlítása




Summary:
- mediapipe for pose estimation
- hdf5 format for data storage
- extracted from videos: skeleton and segmentation masks (maybe key points too?) + trajectory is stored


Next steps:
- find fitiing software architecture and design pattern
- start coding frontend
- get two similar videos and try DTW
- connect fronend and backend






architecture: MVC + Service layer


Technically, you are moving toward what is known as N-Tier Architecture (specifically 3-Tier or 4-Tier) or, more modernly, the Domain-Driven Design (DDD) Lite approach.

While there isn't one single "catchy" name for "MVC + Services," it is most commonly referred to by architects as:

1. Service-Oriented MVC (or "The Service Layer Pattern")
This is the most direct name. You are taking the Controller from MVC and delegating its "work" to a Service Layer.

The Controller: Handles the "Protocol" (HTTP, JSON, Status Codes).

The Service: Handles the "Domain" (The actual dance comparison logic).

2. Hexagonal Architecture (Ports and Adapters)
If you keep going down this path of "clean code," you’ll end up here. The idea is that your Core Logic (the dance scoring math) sits in the center, and everything else (the Database, the React Frontend, the AI Frameworks) are just "adapters" that plug into it.

3. N-Tier Architecture
This is the "classic" enterprise term.

Presentation Tier: Your React Frontend.

Logic/Service Tier: Your Python services (The AI and math).

Data Tier: Your Database (PostgreSQL/MongoDB).



In a standard MVC, the Controller is like a waiter who also cooks the food. In your Service-based architecture:

The Controller is the Waiter (takes your order/video, brings the bill/score).

The Service is the Chef (knows exactly how to use the "MediaPipe" stove and the "DTW Math" knife).

This separation is vital for you because AI models change fast. Last year it was MediaPipe; this year it’s YOLO11x; next year it might be something else. With a Service Layer, you don't have to rewrite your whole app—you just swap the "Chef."


Feature			Standard MVC				MVC + Service Layer
Code Organization	Logic is trapped in Controllers		Logic is isolated and reusable
Testing			Hard (must mock HTTP requests)		Easy (test math functions directly)
Scalability		"Fat" and messy over time		Scalable and "Clean"
AI Integration		Hard to swap models			Easy to "Plug and Play"





The 3-Tier architecture breakdown

Tier 1: Presentation Tier (The Face)
- What it is: The code the user actually sees and touches.
- Your Project: Your React frontend.
- Responsibility: Recording the user's dance, displaying the "Teacher" video, and showing the final score/feedback. It doesn't know how the score is calculated; it just asks the next tier for the result.

Tier 2: Logic/Application Tier (The Brain)
- What it is: The "Middleman" where all the calculations happen.
- Your Project: Your Python/FastAPI/Flask backend.
- Responsibility: This is where your Services live. It receives the video files, runs them through YOLO or MediaPipe, calculates the pose similarity, and decides if the student "passed."

Tier 3: Data Tier (The Memory)
- What it is: Where data is stored permanently.
- Your Project: Your PostgreSQL or MongoDB database.
- Responsibility: Saving user profiles, storing "high scores," and keeping the pre-processed "Teacher" pose data so you don't have to re-calculate it every time.



Think of it this way: N-Tier is the physical map (where the code lives), and MVC + Service is the internal organization (how the code is written).

N-Tier Level	MVC + Service Component		Logic in your Dance AI
Presentation	View (React)			Upload Button, Video Player, Score Graph.
Logic		Controller + Service		The API endpoint that says "Calculate Score" and the ComparisonService that does the math.
Data		Model				The database table for DanceMove and UserResult.



conclusion: I don't need the service component (no database for model, so the logic is in model), so only MVC



MVC architecture:
- model: python "backend" (comparison + feedback generation)
- view: react frontend (components + api)
- controller: python "glue" (flask/fastapi + video processing)



frontend: Vite

running:
- navigate to view folder
- npm install
- npm run dev








Comparison methods



1. Prioritize Skeleton over Mask

In Hip Hop, the Skeleton (MediaPipe) is your "Ground Truth." Because MediaPipe uses heatmaps to predict where the actual joints are inside the clothes, it is much more reliable than the mask.The Strategy: Use the Mask only to check for "Energy" and "Extension" (how much space they occupy), but use the Skeleton for the "Accuracy Score."

Weighted Scoring: $$Total Score = (0.8 \times \text{Skeleton Score}) + (0.2 \times \text{Mask IoU})$$

By giving the mask a lower weight, the baggy clothes won't tank the student's grade.



2. Use "Joint-Centric" Mask Cropping

Instead of comparing the whole body mask at once (where a baggy pant leg looks like a huge error), compare local regions around the joints.

- Take the $(\text{x, y})$ coordinate of the Elbow.
- Crop a small $100\times100$ pixel square around that elbow in both the teacher's and student's masks.
- Compare the relative motion of that crop rather than the static shape. If both "blobs" move in the same direction at the same speed, the score remains high, even if one blob is "fuzzier" due to a sweatshirt.



3. The "Temporal Derivative" Trick (Motion over Shape)

Instead of comparing Mask A to Mask B, compare the Change in Mask A ($\Delta A$) to the Change in Mask B ($\Delta B$).

If the teacher moves their arm up, their mask changes in a specific way.
If the student moves their arm up, their baggy sleeve will also move up.

By comparing the velocity of the pixels (Optical Flow) rather than the static outline, the "extra" cloth becomes irrelevant because the movement vector is the same.



4. Normalizing the Mask "Volume"

To prevent a student in a tight shirt from failing against a teacher in a puffer jacket, you can normalize the area:

Calculate the total area (pixel count) of the teacher's mask ($A_t$) and the student's mask ($A_s$).
Before running your IoU comparison, scale the student's mask by the ratio $\frac{A_t}{A_s}$.

This effectively "inflates" or "deflates" the student's silhouette to match the teacher's visual volume before checking for positional overlap.



5. AI "Style" Filtering

If you want to use an AI model here, you can train a simple Linear Regressor or a Small Neural Network that learns to ignore "noise."

- Input: Skeleton distances + Mask IoU.
- Target: A "Human-labeled" score of how good the dance move was.
- Result: The AI will eventually learn that when the Skeleton is correct but the Mask is slightly off, it should ignore the Mask error (associating it with clothing/noise).







This is the "Holy Grail" of dance AI: distinguishing between Technical Errors (a missed step) and Stylistic Expression (a different "vibe" or "swagger"). In Hip Hop, the "bounce" or "groove" might be different, but the core movement remains.

To avoid penalizing style, you need to move away from "Hard Matching" (Pixel A must be at Point B) and move toward Relative Relationships and Temporal Consistency.



1. Geometric Ratios (Style-Agnostic)

Instead of checking the absolute $(x, y)$ position of a hand, check the Relationship between joints.

The Concept: Style changes the extension, but physics dictates the alignment.Technical Implementation: Calculate the ratio of distances. For example, the distance between the left and right wrist relative to the shoulder width.

Why it works: If a student dances "smaller" (style), their ratios will still match the teacher’s "big" movements, even if the absolute coordinates are 20% different.



2. Dynamic Time Warping (DTW) with Soft Windows

You are already using DTW, which is great for timing. To account for style, don't just look at the Global Distance; look at the Cost Matrix.

The Logic: A "Style Difference" usually results in a constant, small offset across many frames. A "Mistake" usually results in a sharp, sudden spike in the distance.

The Filter: Ignore the "Background Noise" of the distance. Only trigger a "Mistake" feedback if the error exceeds a Rolling Standard Deviation.

If the student is consistently 10% off (Style), it’s fine. If they suddenly jump to 40% off for 5 frames (Mistake), flag it.



3. Latent Space Comparison (The "AI" Way)

This is where you can use a neural network. Instead of comparing raw skeleton data, you train an Autoencoder.

Train: Feed the AI thousands of varied dance movements. The AI learns to compress these into a "Latent Space" (a mathematical summary of the "essence" of a move).

Compare: Encode both the Teacher and the Student into this Latent Space.

The Result: The AI naturally learns to ignore "noise" (like slightly different arm angles or baggy hoodies) and only sees the "Core Feature" of the move. If the Student and Teacher land in the same area of the Latent Space, they are doing the same move, regardless of style.



4. Separating "Path" from "Pose"

In Hip Hop, the Path an arm takes is often more important for style, while the Final Pose is the technical requirement.

The Technique: Use Velocity Vectors. Compare the direction of movement.

Feedback Logic: * Correct Direction + Different Position = Style/Expression.

Wrong Direction + Different Position = Technical Error.



5. Implementation Strategy: The "Tolerance Buffer"

You can implement a "Clean" coding solution for this without a heavy AI model by using a Joint-Specific Tolerance Table:

Joint	Tolerance (Degrees)	Why?
Hips	Low (5°)		The core of the groove; must be precise.
Wrists	High (25°)		High stylistic variation in hand "flourish."
Knees	Medium (15°)		Important for the "Bounce" but varies by height.



To avoid penalizing style:

- Normalize everything by the person's own body proportions.
- Ignore small, constant offsets (Style).
- Target sudden "Spikes" in error (Mistakes).





1. Step-by-Step Comparison Architecture

To get precise feedback while allowing for hip-hop "style," build your model in this sequence:

- Phase A: Temporal Alignment (The Foundation)

Use your raw (N, 33, 4) skeletal data and DTW.
Why: You cannot compare masks or trajectories until you know which frame in the student video matches which frame in the teacher video.
Method: Perform DTW on a "simplified" skeleton (just the 12 main joints: shoulders, hips, knees, ankles, elbows, wrists). This gives you an Alignment Map.

- Phase B: Localized Geometric Comparison (Technical Accuracy)

Instead of comparing $(x, y)$ coordinates directly (which fails if people have different limb lengths), compare Joint Angles.
The Math: Use the Law of Cosines on your skeletal landmarks to calculate angles for elbows, knees, and hips.The "Style" Filter: Apply the Tolerance Buffer here.
Example: If a Hip Hop "bounce" (knee angle) is within 15° of the teacher, give it a 100% score. Only penalize if they are "stiff" (0° change).

- Phase C: Global Shape Comparison (The Mask)

Use your dance_masks.h5 for "Volume" and "Extension" checks.
Method: Calculate the IoU (Intersection over Union) for every aligned frame pair.
Refinement: Since you have baggy clothes, don't just subtract masks. Calculate the Centroid of the mask. If the student's mask centroid is shifting at the same velocity as the teacher's, they have the "groove" right, even if the mask shape is bulkier.

- Phase D: Trajectory Analysis (The Footwork)

Use your trajectory (N, 2) data.
Method: Compare the path shape on the floor.
Logic: A student might be smaller, so normalize the trajectory by the "Standard Height" you used for the masks. Use Cosine Similarity on the velocity vectors of the trajectory to see if they are moving in the right direction.


2. Weighting the Scores for Hip Hop

Because Hip Hop is about "vibe" as much as "steps," your final score should not be a simple average.

Component	Weight	Purpose
Joint Angles	50%	Ensures the "bones" are in the right place.
Trajectory	30%	Ensures they are moving across the floor correctly.
Mask IoU	20%	Ensures they are "dancing big" (extension).


3. How to Generate the Feedback

Since you don't want an autoencoder, you can use a Rule-Based AI to generate text.

The "Score-to-Text" Logic:
Find the Spike: Identify the 3 frames with the worst "Joint Angle" scores.
Identify the Culprit: See which joint was the furthest off (e.g., "Left Knee").
Generate Feedback:
- Condition: Knee angle > Teacher angle + 30°.
- Text: "You're staying too upright during the drop. Try to get lower in your bounce!"
- Condition: Trajectory velocity < Teacher velocity.
- Text: "Your steps are a bit hesitant. Commit more to the sideways slide!"


4. Summary of Data Usage

raw landmarks: Use to calculate Angles (Style-invariant).
trajectory: Use to calculate Footwork/Direction (Spatial accuracy).
masks: Use to calculate Energy/Extension (Visual impact).






The Better Path: The "Scoring Engine"

Since you have your data in .h5 files and want to avoid the headache of building a custom neural network, you should build a Scoring Engine using Python's math libraries. This is faster, easier to debug, and gives you total control.


How to Build Your Comparison "Model" (Without Training)

You don't need to "train" a model to see if two people are doing the same thing. You just need to quantify the difference.


1. The Skeletal "Pose Score"

Use your raw (N, 33, 4) data. Instead of comparing coordinates, calculate Vector Angles.
Vector A: Shoulder to Elbow.
Vector B: Elbow to Wrist.
The Angle: The dot product of these two vectors gives you the exact bend of the arm.

Why? Angles are the same whether the person is 5 feet tall or 6 feet tall.


2. The Mask "Volume Score"

Use your dance_masks.h5. Since they are already centered and normalized, you can use Structural Similarity Index (SSIM) or IoU.

Logic: If the teacher's mask has a large "blob" in the upper-right quadrant (arm extension) but the student's doesn't, the score drops.


3. The "AI" Feedback Layer

This is where the "AI" part happens. Take your calculated scores (e.g., elbow_angle_error: 15.2, mask_iou: 0.65) and send them to a Large Language Model (LLM) like Gemini or GPT.

Input to LLM: "The student is doing a hip hop slide. Their knee bend is 10 degrees shallower than the teacher's, and their mask overlap is only 60% because their arms are tucked in.

"AI Output: "Great energy! To catch the true hip hop vibe, try to sink deeper into your knees and really reach out with your arms to fill the space."


Summary of the "No-Train" Architecture

Component	Data Used		Method
Timing		Raw Landmarks		DTW (Dynamic Time Warping)
Accuracy	Raw Landmarks		Joint Angle Trigonometry
Power/Extension	Binary Masks		IoU (Intersection over Union)
Feedback	Calculated Scores	LLM Prompting (Text generation)




Summary: components of comparison
- timing - DTW
- skeleton - joint angles
- mask - see below (line 661)
- trajectory - similar directions
- style - tolerance buffers





Hu Moments (Shape Fingerprints)

Hu Moments are a set of 7 numbers calculated from the "moments" (weighted averages of pixel intensity) of a shape. They are famous for being invariant to scale, rotation, and translation.

How it works: It treats the mask like a physical object with a center of mass and distribution.

Why it's deep: The first few moments describe the "bulk" of the shape (is it long or wide?), while the later moments describe finer details.

The Hip Hop Logic: In styles like Popping or Breaking, the silhouette's "weight distribution" is key. Hu Moments can tell you if the "center of gravity" of the two masks is in the same relative place, regardless of how much the fabric is flapping around.



Elliptical Fourier Descriptors (EFD)

If Hu Moments look at the "mass" of the mask, EFD looks at the contour (the outline). It breaks the outline down into a series of "harmonics" (sine and cosine waves).

The Logic: Low-frequency harmonics represent the general shape (the "pose"). High-frequency harmonics represent the noise (the wrinkles in a baggy shirt).

The Hip Hop Strategy: To ignore the "baggy clothes noise," you can discard the high-frequency harmonics and only compare the first 5–10 harmonics. This effectively "smooths out" the clothes, leaving only the fundamental dance posture behind.



Mask-to-Symmetry Ratios

This is a custom heuristic rather than a standard math formula, but it’s incredibly effective for dance.

Approach: Divide the mask into four quadrants based on the MediaPipe center (the "mid-hip" point). Compare the ratio of mask pixels in the top-left vs. top-right.

The Weight/Height Factor: Ratios are "unitless." Even if one dancer is heavier, the ratio of their arm-spread to their torso-width should remain somewhat consistent with the other dancer if they are hitting the same shape.

The Method: You split the mask along the vertical axis (using the MediaPipe spine/nose as the center).

Calculations:

Left-to-Right Ratio: Area(Left) / Area(Right).

Top-to-Bottom Ratio: Area(Torso-up) / Area(Legs-down).

Why it works for Height/Weight: Ratios are "normalized" by nature. A 200lb dancer and a 120lb dancer hitting a "T-pose" will have nearly identical Left-to-Right ratios (~1.0), even though their raw pixel counts are vastly different.



Distance Transform Mapping (The "Fuzzy" Overlap)

Standard mask comparison (Dice/IoU) is binary: a pixel either overlaps or it doesn't. This is too harsh for Hip Hop, where a baggy sleeve might be 10 pixels off, but the arm position is correct.

How it works: You take Dancer A's mask and convert it into a Distance Map. In this map, pixels inside the mask are "brightest" at the center of the limb and fade to "dark" at the edges.

The Comparison: You multiply Dancer B's binary mask by Dancer A's Distance Map.

Why it's better: If Dancer B is slightly off, they still land in the "fading" zone of Dancer A's map and get partial credit. It rewards "being close" rather than requiring perfection.



Mask-Based Optical Flow (The "Power" Metric)

Since Hip Hop is about texture and bounce, you can calculate the Optical Flow (pixel movement between frames) specifically inside the segmentation masks.

Why? Two dancers might hit the same final pose, but one might "hit" it harder (higher velocity). Comparing the average magnitude of motion within the mask helps you quantify the "energy" or "power" of the move, which mask-matching alone misses.

Hip Hop is defined by its dynamics—hits, pops, and flows. Mask matching only looks at the "stills." Optical Flow captures the velocity of the pixels between frames.

How it works: Instead of calculating flow for the whole video, you only calculate it for pixels inside the segmentation mask.

The Comparison: You calculate the average "Energy" (magnitude of velocity) of the mask in Video A vs. Video B.

The Hip Hop Logic: If the move is a "Pop," the mask should show a high-velocity spike. If one dancer "hits" it and the other just "moves" to it, their masks might look identical, but their Optical Flow energy will be totally different.



conclusion:
- hu moments: center of gravity
- efd: eliminating noise, clear shapes
- dtm: shape similarity
- optical flow: vibe check




This is a very sophisticated pipeline. By combining these four, you aren’t just comparing pixels; you are comparing geometry (Hu), morphology (EFD), spatial accuracy (DTM), and dynamics (Flow).

For Hip Hop specifically, this "stack" addresses the baggy clothes problem by stripping away high-frequency noise while retaining the "power" of the movement.


1. Hu Moments (The "Anchor")

Role: Finding the Center of Gravity (CoG) and global orientation.

Why it works: In Hip Hop, the "pocket" or "groove" is often defined by where the dancer's weight is shifted. Even with baggy clothes, the 1st Hu Moment (Centroid) remains remarkably stable.

Pro-Tip: Use the distance between the MediaPipe Mid-Hip and the Hu-Centroid. If the Hu-Centroid is significantly lower than the hip, you know the dancer is in a deep "plie" or crouch, which is a key stylistic marker.


2. EFD (The "De-Noiser")

Role: Smoothing the silhouette to ignore fabric folds.

Why it works: Baggy hoodies create jagged edges in a raw mask. EFD allows you to reconstruct a "clean" version of the dancer.

Pro-Tip: Use only the first 5 to 8 harmonics. This will turn a wrinkly oversized shirt into a smooth, curved shape that represents the actual limb position.


3. DTM (The "Fair" Judge)

Role: Calculating the similarity score of the smoothed shapes.

Why it works: After EFD cleans the shape, DTM ensures that "close is good enough."

Pro-Tip: Instead of a linear distance transform, use a Gaussian-weighted Distance Map. This makes the "sweet spot" (the center of the limb) much more valuable than the edges.


4. Optical Flow (The "Soul")

Role: Measuring the "Pop" and "Lock."

Why it works: Two dancers can have the exact same DTM score, but if Dancer A moves into the pose twice as fast as Dancer B, the "vibe" is different.

Pro-Tip: Calculate "Flow Acceleration" (the change in flow velocity). Hip Hop "hits" are characterized by a sudden acceleration followed by an instant stop (zero velocity).



The order of operations is vital. You should run the EFD smoothing FIRST, then use that "clean" mask to generate your DTM and Hu Moments. If you calculate Hu Moments on the raw, noisy mask, the "flapping" of baggy clothes might jitter the results.

The "Logic Flow" should look like this:
1. Extract Raw Mask (MediaPipe).
2. Smooth via EFD (Low-pass filter).
3. Generate DTM from the Smoothed Mask of Dancer A.
4. Compare Smoothed Mask of Dancer B against Dancer A's DTM.
5. Weight the result by the Optical Flow "Energy" and Skeleton proximity.







video processing path:
-> POST /dance_videos
-> Save files to uploaded_videos/
-> Generate session dir: data/YYYYMMDD_HHMMSS/
-> asyncio.to_thread: data_extraction(teacher, dir, 'teacher')
-> teacher_data.h5 + teacher_masks.h5
-> asyncio.to_thread: data_extraction(student, dir, 'student')
-> student_data.h5 + student_masks.h5
-> SSE: Processing complete




correction: applying center of gravity analysis to the skeleton instead of hu moments on the mask - much simpler and easier






solving issues with matching the start of the choreography:

1. Audio cross-correlation  →  sync the two videos (global offset)
2. Motion energy detection  →  find active range in EACH video
3. Take the INTERSECTION    →  keep only frames where BOTH are dancing
4. Feed into DTW            →  align & compare
